{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlGsu74PRze0"
   },
   "source": [
    "[<h1> FIT5197 Assignment 3 Semester 2, 2020 </h1>](https://lms.monash.edu/mod/assign/view.php?id=7560449)\n",
    "\n",
    "---\n",
    "Authors: Dan Nguyen, Yun Zhao\n",
    "\n",
    "Admins (Competition): Dr. Levin Kuhlmann, Yun Zhao, Anil Gurbuz\n",
    "\n",
    "Proofreaders: Dr. Levin Kuhlmann, Yun Zhao, and other tutors \n",
    "\n",
    "Date: Oct 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h1> Assignment Instruction </h1>](https://lms.monash.edu/mod/assign/view.php?id=7560449)\n",
    "\n",
    "<span style=\"color:red\"> Please read through the instructions carefully, by submitting the assignment, you are considered to have read all the instructions carefully and be aware of the penalties that entail. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-lggSuURze1"
   },
   "source": [
    "<h1>Part 1: Regression (50 Marks)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ar--DpysOWJd"
   },
   "source": [
    "This part is about regression. Specifically, you will be ``predicting the fuel efficiency`` of a car (in kilometers per litre) based on its characteristics. This is a practical problem as Australia is one of the largest automobile markets in the world; thus, correctly predicting the fuel efficiency is necessary to control emission rates to the environment.\n",
    "\n",
    "The dataset has many observations and predictors obtained from many retailers for car models available for sale from 2017 to 2020. The target variable is the fuel efficiency of the car measured in kilometers per litre. The higher this value, the better the fuel efficiency of the car. \n",
    "\n",
    "PleaseProvide working/R code/justifications for each of these questions as required.\n",
    "\n",
    "$\\textbf{Note:}$ If not explicitly mentioned, libraries are not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from students' side\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"RegressionTrain.csv\")\n",
    "test <- read.csv(\"RegressionTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Please skip (don't run) this if you are a student\n",
    "# Read in the data from marking tutors' side (ensure no cheating!)\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"../data/RegressionTrain.csv\")\n",
    "test <- read.csv(\"../data/RegressionTest.csv\")\n",
    "label <- read.csv(\"../data/RegressionTestLabel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epC4XmxOWJe"
   },
   "source": [
    "<h2> Question 1 (5 Marks) </h2>\n",
    "\n",
    "Fit a $\\textbf{multiple linear model}$ to the fuel efficiency data using the ``train`` dataset. By checking the summary information, which predictors/variables do you think are possibly associated with fuel efficiency (use ``0.05`` significant level), and why? Which ``three predictors/variables`` appear to be the strongest predictors of fuel efficiency, and why? \n",
    "\n",
    "$\\textbf{Note}$: You don't have to worry about categorical variables here since R can deal with this automatically, focus your efforts on interpretation. Additionally, when explaining why features are strongly associated with the target, please refrain giving one or two sentences answers, these answers are not descriptive enough and will result in deduction of marks. Finally, please name the model here ``lm.fit`` for future marking purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "$\\textbf{YOUR ANSWER HERE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Multiple Linear regression model\n",
    "# Target variable : Comb.FE\n",
    "# Predictors : all the columns\n",
    "lm.fit <- lm(Comb.FE ~ . , data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ ., data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0256 -0.9978 -0.0644  0.7006 11.3941 \n",
       "\n",
       "Coefficients:\n",
       "                           Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)              -1.783e+02  8.587e+01  -2.076  0.03809 *  \n",
       "Model.Year                9.640e-02  4.255e-02   2.266  0.02363 *  \n",
       "Eng.Displacement         -1.364e+00  1.025e-01 -13.306  < 2e-16 ***\n",
       "No.Cylinders              4.644e-02  6.769e-02   0.686  0.49282    \n",
       "AspirationOT             -3.452e-01  6.352e-01  -0.543  0.58693    \n",
       "AspirationSC             -9.197e-01  2.282e-01  -4.031 5.85e-05 ***\n",
       "AspirationTC             -1.303e+00  1.288e-01 -10.111  < 2e-16 ***\n",
       "AspirationTS             -1.149e+00  4.945e-01  -2.323  0.02035 *  \n",
       "No.Gears                 -1.307e-01  2.995e-02  -4.364 1.37e-05 ***\n",
       "Lockup.Torque.ConverterY -8.243e-01  1.117e-01  -7.377 2.78e-13 ***\n",
       "Drive.SysA               -8.339e-02  1.521e-01  -0.548  0.58356    \n",
       "Drive.SysF                1.441e+00  1.711e-01   8.419  < 2e-16 ***\n",
       "Drive.SysP               -2.400e-01  2.980e-01  -0.805  0.42087    \n",
       "Drive.SysR                4.328e-02  1.476e-01   0.293  0.76938    \n",
       "Max.Ethanol              -7.076e-03  2.967e-03  -2.385  0.01722 *  \n",
       "Fuel.TypeGM               5.706e-01  4.173e-01   1.368  0.17169    \n",
       "Fuel.TypeGP               4.093e-01  1.369e-01   2.990  0.00284 ** \n",
       "Fuel.TypeGPR              1.363e-01  1.401e-01   0.973  0.33096    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.598 on 1382 degrees of freedom\n",
       "Multiple R-squared:  0.6628,\tAdjusted R-squared:  0.6586 \n",
       "F-statistic: 159.8 on 17 and 1382 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the key statictics of the Linear model with respect to the predictors\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which predictors/variables do you think are possibly associated with fuel efficiency (use 0.05 significant level), and why**\n",
    "\n",
    "**Answer**\n",
    "\n",
    "By looking at the statistics given by `summary()` function, we can see that 6 predictors are possibly associated with Fuel efficiency. \n",
    "The 6 possibly associated predictors are\n",
    "1. Eng.Displacement\n",
    "2. AspirationSC\n",
    "3. AspirationTC\n",
    "4. No.Gears\n",
    "5. Lockup.Torque.ConverterY\n",
    "6. Drive.SysF\n",
    "\n",
    "We can say the above 6 predictors are possibly associated to fuel efficiency because the t values are relatively far from 0, suggesting to reject the null hypothesis that `beta` is 0. And also, the t value is relatively large compared to the standard error, which indicate a relationship exists between these 6 predictors and Fuel efficiency.\n",
    "\n",
    "Finally looking at the last coefficeint `Pr(>|t|)`, we see for the 6 prdeictors the probability of observing any value equal or larger than t is really low. This supports our inference that the 6 predictors are related to target variable Fuel efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which three predictors/variables appear to be the strongest predictors of fuel efficiency, and why?**\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The 3 predictors/variables that appear to be the strongest predictors of fuel efficiency are \n",
    "1. Eng.Displacement\n",
    "2. AspirationTC\n",
    "3. Drive.SysF\n",
    "\n",
    "We can say this because the value of `Pr(>|t|)` suggsets that the probablity of seeing a t value as large as this is really low. Means that we can reject the null hypothesis that `beta` is 0, thus suggesting that these 3 predictors are the most strongly related to Fuel Efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUu5eFpOWJi"
   },
   "source": [
    "<h2> Question 2 (5 Marks) </h2>\n",
    "\n",
    "Describe/discuss the effect that the year of manufacture ``(Model.Year)`` variable appears to have on the mean ``fuel efficiency``. Additionally, describe/discuss the effect that the number of gears ``(No.Gears)`` variable has on the mean ``fuel efficiency`` of the car.\n",
    "\n",
    "$\\textbf{Note}$: This asks for your descriptions, please refrain from using one or two lines to describe/discuss the effect. Keep answers to be 4 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ Model.Year, data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-5.4805 -1.8537 -0.4876  1.3016 15.7799 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)\n",
       "(Intercept)  -9.042125 144.142701  -0.063    0.950\n",
       "Model.Year    0.009656   0.071418   0.135    0.892\n",
       "\n",
       "Residual standard error: 2.736 on 1398 degrees of freedom\n",
       "Multiple R-squared:  1.308e-05,\tAdjusted R-squared:  -0.0007022 \n",
       "F-statistic: 0.01828 on 1 and 1398 DF,  p-value: 0.8925\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit a Multiple Linear regression model\n",
    "# Target variable : Comb.FE\n",
    "# Predictor : Year\n",
    "lm.fit.Year <- lm(Comb.FE ~ Model.Year , data = train)\n",
    "summary(lm.fit.Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Looking at the above summary of Linear model of Comb.FE and the Model.Year we see that the t value is 0.135, which is really small. This small t value supports the null hypothesis of of `beta` being 0.\n",
    "Thus we can conclude that no relationship exists between `Model.Year` and `fuel efficiency`.\n",
    "\n",
    "We now look at the probablity of getting a t value greater than 0.135.\n",
    "\n",
    "We know that when p value < 0.05, there exists a relation between predictor and the target variable.\n",
    "\n",
    "In this case `Pr(>|t|)` value is 0.892 > 0.05, this means that the variable `Model.Year` has no effect on `fuel effiency`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ No.Gears, data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-5.6520 -1.6453 -0.2262  1.5197 15.1466 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) 14.95914    0.27586   54.23   <2e-16 ***\n",
       "No.Gears    -0.64691    0.03838  -16.86   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.494 on 1398 degrees of freedom\n",
       "Multiple R-squared:  0.1689,\tAdjusted R-squared:  0.1683 \n",
       "F-statistic: 284.1 on 1 and 1398 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit a Multiple Linear regression model\n",
    "# Target variable : Comb.FE\n",
    "# Predictor : No.Gears\n",
    "lm.fit.No.Gears <- lm(Comb.FE ~ No.Gears , data = train)\n",
    "summary(lm.fit.No.Gears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above summary of Linear model of Comb.FE and the No.Gears we see that the t-value is -16.86, which is large. This large t-value suggests to reject the null hypothesis of of `beta` being 0.\n",
    "Thus we can conclude that there exists a relationship between `No.Gears` and `fuel efficiency`.\n",
    "\n",
    "We now look at the probablity of getting a t-value greater than 16.86.\n",
    "\n",
    "We know that when p value < 0.05, there exists a relation between predictor and the target variable.\n",
    "\n",
    "In this case `Pr(>|t|)` value is 2e-16 < 0.05, this means that the variable `Model.Year` is related to `fuel effiency`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XyKtwfOWJj"
   },
   "source": [
    "<h2> Question 3 (5 Marks) </h2>\n",
    "\n",
    "Apply the stepwise selection procedure with the $\\textbf{BIC}$ penalty to prune out potentially less significant variables. Write down the final regression equation obtained after pruning, please keep the values of the parameter coefficients to 2 decimal places. Finally, also describe the pruned model.\n",
    "\n",
    "$\\textbf{Note}$: please don't change the default direction ``both`` in the step function, this is so that we can check your work easily. Additionally, please name this model ``sw.fit``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "$\\textbf{YOUR ANSWER HERE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your code contains a unicode char which cannot be displayed in your\n",
      "current locale and R will silently convert it to an escaped form when the\n",
      "R kernel executes this code. This can lead to subtle errors if you use\n",
      "such chars to do comparisons. For more information, please see\n",
      "https://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ Eng.Displacement + Aspiration + No.Gears + \n",
       "    Lockup.Torque.Converter + Drive.Sys + Max.Ethanol, data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0743 -0.9760 -0.0349  0.6566 11.3971 \n",
       "\n",
       "Coefficients:\n",
       "                          Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)              16.196874   0.282901  57.253  < 2e-16 ***\n",
       "Eng.Displacement         -1.277173   0.043418 -29.416  < 2e-16 ***\n",
       "AspirationOT             -0.100081   0.626276  -0.160 0.873060    \n",
       "AspirationSC             -0.699137   0.213768  -3.271 0.001100 ** \n",
       "AspirationTC             -1.144227   0.107302 -10.664  < 2e-16 ***\n",
       "AspirationTS             -1.122104   0.481471  -2.331 0.019919 *  \n",
       "No.Gears                 -0.113537   0.029183  -3.891 0.000105 ***\n",
       "Lockup.Torque.ConverterY -0.825285   0.110202  -7.489 1.23e-13 ***\n",
       "Drive.SysA                0.035013   0.145617   0.240 0.810020    \n",
       "Drive.SysF                1.480191   0.166847   8.872  < 2e-16 ***\n",
       "Drive.SysP               -0.323201   0.292617  -1.105 0.269560    \n",
       "Drive.SysR                0.093779   0.146329   0.641 0.521710    \n",
       "Max.Ethanol              -0.008344   0.002934  -2.843 0.004528 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.605 on 1387 degrees of freedom\n",
       "Multiple R-squared:  0.6586,\tAdjusted R-squared:  0.6557 \n",
       "F-statistic:   223 on 12 and 1387 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform Stepwise selection procedure with the  𝐁𝐈𝐂  penalty on the Linear model\n",
    "sw.fit <- step(lm.fit,k=log(nrow(train)),trace=0,direction = c(\"both\"))\n",
    "\n",
    "# show the statistics of the prunned linear model\n",
    "summary(sw.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "After applying the stepwise selection procedure with the  𝐁𝐈𝐂  penalty, we can see that the below 6 predictors are important:\n",
    "1. Eng.Displacement\n",
    "2. AspirationSC\n",
    "3. AspirationTC\n",
    "4. No.Gears\n",
    "5. Lockup.Torque.ConverterY\n",
    "6. Drive.SysF\n",
    "\n",
    "These 6 predictors are important as the p value is low for them, which suggest to reject the null hypothesis of `beta` being 0.\n",
    "\n",
    "So the final regression equation after using Stepwise selection procedures using BIC penalty is as follows :\n",
    "\n",
    "y = -(1.27)Eng.Displacement -(0.69)AspirationSC -(1.14)AspirationTC -(0.11)No.Gears -(0.82)Lockup.Torque.ConverterY + (1.48)Drive.SysF + 16.196874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izq8RU0lOWJn"
   },
   "source": [
    "<h2> Question 4 (5 Marks) </h2>\n",
    "\n",
    "Say we are going to buy a new car and we want to improve the fuel efficiency of our new car, what does this ``BIC model`` suggest we should do? Provide a detailed answers of at least ``150 words``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "The prunned Linear model gives us the important variables and their respective coefficients to calculate the final Fuel efficiency.\n",
    "\n",
    "The linear equation has the form y = mx + c, where y: Fuel efficiency, m: coefficient, x: predictor and c: intercept\n",
    "\n",
    "Final equation after using Stepwise selection procedures using BIC penalty:\n",
    "\n",
    "y = -(1.27)Eng.Displacement -(0.69)AspirationSC -(1.14)AspirationTC -(0.11)No.Gears -(0.82)Lockup.Torque.ConverterY + (1.48)Drive.SysF + 16.196874\n",
    "\n",
    "**Evaluating the Categorical variables:**\n",
    "**1. Aspiration :** As aspiration is a categorical variables, we evaluate each value namely N, TC, SC, OT and TS independently. Looking at the summary of the final linear model, the coefficient of Aspiration OT is the largest of the 5 values of Aspiration. This value of Aspirtaion OT will help maxizmize the fuel efficinecy.\n",
    "\n",
    "**2. Lockup.Torque.Converter:** The summary of linear model suggests to have a car with Lockup Torque Converter as this will help maximize the Fuel efficiency.\n",
    "\n",
    "**3. Drive.Sys :** The model suggests to have a car with Front wheel drive to have maximum fuel efficiency. We can conclude this looking at the coeffiecient of the Drive.Sys.F. It is the highest amongst the other values, thus will contribute more to the final equation.\n",
    "\n",
    "**4. Fuel.Type:** Looking at the final quation of the prunned model, we can see that the Fuel.Type variable is not pressent. This means the fuel type of the car is not important and does not contribute the fuel efficiency. \n",
    "\n",
    "**Evaluating the Numerical variables:**\n",
    "\n",
    "**1. Model.Year:** As the final linear equation after prunning does not have Model.Year in it, we can say that Year is not important for Fuel efficiency.\n",
    "\n",
    "**2. Eng.Displacement :** The coefficient of Eng.Displacement is -1.277173, which means higher the value of Eng.Displacement, lower will be the Fuel Efficiency. Hence the model suggests to have lower Eng.Displacement.\n",
    "\n",
    "**3. No.Cylinders :** As the final linear equation after prunning does not have No.Cylinders in it, we can say that No.Cylinders is not important for Fuel efficiency.\n",
    "\n",
    "**4. No.Gears :** The coefficient of No.Gears is -0.113537, which means higher the No.Gears, lower will be the Fuel Efficiency. Hence the model suggests to have lower No.Gears.\n",
    "\n",
    "**5. Max.Ethanol :** The coefficient of Max.Ethanol is -0.008344, which means higher the percentage of Max.Ethanol, lower will be the Fuel Efficiency. Hence the model suggests to have lower % of Ethanol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhFKnrlcOWJo"
   },
   "source": [
    "<h2> Question 5 (5 Marks)</h2>\n",
    "\n",
    "Imagine that you are looking for a new car to buy to replace your existing car. Use the $\\textbf{test}$ dataset to inspect the first car fuel efficiency and see whether it is a good fit for you or not.\n",
    "    \n",
    "    (a) Use your BIC model to predict the mean fuel efficiency for this new car. Provide a 95% confidence interval for this prediction. [2 mark]\n",
    "    (b) Following the previous estimation, given that the current car that you own has a mean fuel efficiency of 9.5 km/l (measured over the life time of your ownership), does your model (BIC) suggest that the new car will have better fuel efficiency than your current car? Why? [3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "$\\textbf{YOUR ANSWER HERE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5.a**\n",
    "\n",
    "Expected Mean Fule efficiency of the new car is 9.287257 km/l.\n",
    "\n",
    "The 95% confidence interval for this prediction is [9.052956, 9.521557] km/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>9.287257</td><td>9.052956</td><td>9.521557</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " fit & lwr & upr\\\\\n",
       "\\hline\n",
       "\t 9.287257 & 9.052956 & 9.521557\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| fit | lwr | upr |\n",
       "|---|---|---|\n",
       "| 9.287257 | 9.052956 | 9.521557 |\n",
       "\n"
      ],
      "text/plain": [
       "  fit      lwr      upr     \n",
       "1 9.287257 9.052956 9.521557"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(sw.fit,test[1,],interval = \"confidence\", prediction.interval = 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5.b**\n",
    "\n",
    "Fuel efficiency of current car : 9.5 km/l\n",
    "\n",
    "Estimated fuel efficiency of new car : 9.287257 km/l\n",
    "\n",
    "So, the new car might not have better Fuel efficiency compared to the current car according to our Linear model. We can be 95% sure about this prediction value. Also, the upper limit for this prediction is 9.521557 km/l, so there is less probobablity of the new car being better than the old car.\n",
    "\n",
    "**Analysis of why the new Car is not better :**\n",
    "\n",
    "**1. Eng.Displacement :** The new car has Eng. Displacement of 3.9. From our linear model we can see that higher this value, lower is the Fuel efficiency.\n",
    "\n",
    "**2. Aspiration :** The new car has aspiration TC. The coeffiecien to of AspirationTC is -1.144227, which suggests that this will decrease the Fuel efficiency of the new car. AspirationTC is the variable that contributes most to the decrease in the Fuel efficiency of the new car.\n",
    "\n",
    "**3. No.Gears :** The new car has 6 gears. According to our model higher no. of gears will decrease the Fuel efficiency.\n",
    "\n",
    "**4. Lockup.Torque.Converter :** The new car does not have Lockup Torque converter, so according to our model, the new car has less Fuel efficiency as it lacks this feature.\n",
    "\n",
    "**5. Drive.Sys :** For good fuel efficiency our model suggests to have Front drive. As the new car has Rear wheels drive, the fuel efficiency is comparitevely less than the one with Front wheel drive.\n",
    "\n",
    "**6. Max.Ethanol :** The new car has Ethanol = 10%, according to our model, less the value better is the Fuel efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 6 (Libraries are allowed) (25 Marks) </h2>\n",
    "\n",
    "As a Data Scientist, one of the key tasks is to build models $\\textbf{most appropriate/closest}$ to the truth; thus, modelling will not be limitted to these steps in the assignment. To simulate for a realistic modelling process, this question will be in the form of a competition among students to find out who has the best model.\n",
    "\n",
    "Thus, You will be graded by the performance of your model compared to your classmates', the better your model, the higher your score. Additionally, you need to write a short paragraph describing/documenting your thought process in this model building process ``(300 words)``. Note that this is to explain to us why you build your current model so that we can verify that you understand the model you build and not just copy from other people.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "``install.packages(\"some package\", repos='http://cran.us.r-project.org')``\n",
    "\n",
    "``library(\"some package\")``\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you 0 marks, our suggestion is for you to use the standard ``R version 3.6.1``\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works Appropriately.\n",
    "\n",
    "$20$ Marks for the model performance in the competition\n",
    "\n",
    "$5$ Marks for logically writing down the thought process in building the final model\n",
    "\n",
    "This is the [link](https://www.kaggle.com/t/0a3c0fc91b074816a6315bb4e9b42602) to the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The main objective of this task is to build a model that most appropriately resembles the true value of Fuel efficiency. This involves predicting the numerical value of Fuel efficiency given the different features of the car. Therefore, it qualifies to be a Regression problem.\n",
    "\n",
    "To start with the Model building process, we first use the Multiple Linear regression model which we prunned using BIC penalty. Uisng this model as the starting point, we run the predictions. This gives a Root mean Square (RMSE) value of around 1.69275.\n",
    "\n",
    "To improve our predictions of Fuel Efficiency, we try out different Regression Algorithms to have a better model that most appropriately predicts the Fuel efficiency.\n",
    "\n",
    "Following are the different algorithms I have tried for this Regression task with the respective approximate RMSE values obatined:\n",
    "\n",
    "1. Recursive Partitioning And Regression Trees - 1.46625\n",
    "2. Linear Regression model (Prunned) - 1.69275\n",
    "3. kNN - 1.44106\n",
    "4. SVM - 1.38612\n",
    "5. Boosted regression trees - 1.55203\n",
    "6. Random Forest - RMSE - 1.25855\n",
    "\n",
    "Looking at the above RMSE values, its best to use Random Forest for this regression task. As Random forest randomly selects the predictors and performs the split, it uses the information of error to have a better guess of the next predictors. This helps improve the performance compared to using a Decision tree algorithm/ \n",
    "\n",
    "To have a robust model, I perform k-fold validation. In this the training data set is divided into k subset (k=10). Before we divide the training dataset into k subsets, I randomly shuffle the training dataset to remove any ordering of the data.\n",
    "\n",
    "For each subset the Regression model is trained using the subset of training data and the model is updated in each iteration. The final model is used for making the prediction of the test data.\n",
    "\n",
    "The parameters tuned for the Random Forest are `ntree` and `mtry`.\n",
    "\n",
    "1. `ntree` : Number of tree to grow in the random forest.\n",
    "2. `mtry` : Number of predictors randomly sampled as potential candidates for each split.\n",
    "\n",
    "`mtry` = 6, because the prunned Linear model suggested we have 6 significant predictors. Also, having large `mtry` leads to overfitting.\n",
    "\n",
    "`ntree` = 100, as the algorithm stabalises and shows no major improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to check the performance of your model\n",
    "rmse <- function(pred.label, truth.label){\n",
    "    # Lower is better\n",
    "    return(sqrt(mean((pred.label - truth.label)^2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/prash/OneDrive/Documents/R/win-library/3.6'\n",
      "(as 'lib' is unspecified)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'caret' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\prash\\AppData\\Local\\Temp\\RtmpOoIq10\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/prash/OneDrive/Documents/R/win-library/3.6'\n",
      "(as 'lib' is unspecified)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'rpart' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\prash\\AppData\\Local\\Temp\\RtmpOoIq10\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/prash/OneDrive/Documents/R/win-library/3.6'\n",
      "(as 'lib' is unspecified)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'randomForest' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\prash\\AppData\\Local\\Temp\\RtmpOoIq10\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "# packages to install\n",
    "install.packages(\"caret\")\n",
    "install.packages(\"rpart\")\n",
    "install.packages(\"randomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'caret' was built under R version 3.6.3\"Loading required package: lattice\n",
      "Warning message:\n",
      "\"package 'lattice' was built under R version 3.6.3\"Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'rpart' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'randomForest' was built under R version 3.6.3\"randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# packages required\n",
    "library(caret)\n",
    "library(rpart)\n",
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Regression model\n",
    "\n",
    "#Randomly shuffle the data\n",
    "train<-train[sample(nrow(train)),]\n",
    "\n",
    "#Create 10 equally size folds\n",
    "folds <- cut(seq(1,nrow(train)),breaks=10,labels=FALSE)\n",
    "\n",
    "#Perform k fold ( k = 10) cross validation\n",
    "for(i in 1:10){\n",
    "    \n",
    "    #Segement training data \n",
    "    testIndexes <- which(folds==i,arr.ind=TRUE)\n",
    "    \n",
    "    # train and test data\n",
    "    testData <- train[testIndexes, ]\n",
    "    trainData <- train[-testIndexes, ]\n",
    "    \n",
    "    # build a model, if doesnt exist\n",
    "    if(i == 1){\n",
    "        fin.mod  <- randomForest(Comb.FE ~ ., data = trainData, ntree=100,mtry = 6)\n",
    "    }\n",
    "    # update the existing model\n",
    "    else{\n",
    "        update(fin.mod , data = trainData)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the common columns between training and test data\n",
    "common <- intersect(names(train), names(test)) \n",
    "\n",
    "for (p in common) \n",
    "{\n",
    "    # make the levels same for factors of the test and training data\n",
    "    if (class(train[[p]]) == \"factor\") \n",
    "    { \n",
    "        levels(test[[p]]) <- levels(train[[p]]) \n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions using the final model\n",
    "pred.label <- predict(fin.mod, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# put this label in a csv file to commit to the Leaderboard\n",
    "write.csv(data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "          \"RegressionPredictLabel.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "RMSE.fin <- rmse(pred.label, label$Label)\n",
    "cat(paste(\"RMSE is\", RMSE.fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-lggSuURze1"
   },
   "source": [
    "<h1>Part 2: Classification (50 Marks)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to work with \"Census Income Dataset\" which was originally donated by Ronny Kohavi and Barry Becker to UCI (University of California, Irvine) in 1996. This is a trimmed dataset used for machine learning students to study classification. \n",
    "\n",
    "This dataset has collected over 40,000 records (we excluded some data in our version) regarding personal yearly income with 12 attributes (predictors). The attributes comprise many aspects of a person that may contribute to the yearly income. You can use summary() function to obtain the attributes information. Your prediction task is to determine whether a person makes over 50K a year.\n",
    "\n",
    "We have splitted the dataset into a trainning and a testing set. There are 27245 records in the training set while 13631 records in the testing set. Besides the 12 predictors, there is one more column named Salary indicating whether a person's yearly income is over 50K. The label information is a seperated file for the testing set and will be used by us to asess your performance later. Note the label TRUE means an individual's yearly salary exceeds 50K while FALSE means an individual's yearly salary is under 50K.\n",
    "\n",
    "$\\textbf{Note:}$ If not explicitly mentioned, libraries are not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from students' side\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"ClassTrain.csv\")\n",
    "test  <- read.csv(\"ClassTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Please skip (don't run) this if you are a student\n",
    "# Read in the data from marking tutors' side (ensure no cheating!)\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"../data/ClassTrain.csv\")\n",
    "test  <- read.csv(\"../data/ClassTest.csv\")\n",
    "label <- read.csv(\"../data/ClassTestLabel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epC4XmxOWJe"
   },
   "source": [
    "##### <h2> Question 1 (10 Marks) </h2>\n",
    "\n",
    "Fit a $\\textbf{Generalized Linear Model (Logistic Regression)}$ to predict level of income (salary) $\\left(\\;\\geq 50\\;\\text{K, or } <50\\;\\text{K}\\;\\right)$ using the ``train`` dataset. Using the results of fitting this model, which predictors do you think are possibly associated with the level of Salary (use ``0.05`` significant level), and why? Which ``three variables`` appear to be the strongest predictors of salary, and why? \n",
    "\n",
    "Furthermore, you can see that you have much more predictors in this part than in the ``linear model`` from Part 1 $\\Rightarrow$ manually checking information is counterproductive. Thus, please write a function to automate these processes $\\textbf{(1)}$ selecting important feature against 0.05 threshold and $\\textbf{(2)}$ Selecting three most important features.\n",
    "\n",
    "$\\textbf{Note}$: You don't have to worry about categorical variables here since R can deal with this automatically, focus your efforts on interpretation. Additionally, when explaining why features are strongly associated with the target, please refrain from giving one or two sentences answers, these answers are not descriptive and will result in a deduction of marks. Finally, please name the model here ``glm.fit`` and have the parameter in the model set to ``family = binomial``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "$\\textbf{YOUR ANSWER HERE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    }
   ],
   "source": [
    "# Building a Generalized Linear Model for Classification\n",
    "# Predictors : all columns\n",
    "# Target variable : Salary\n",
    "glm.fit = glm(Salary~. , data=train, family=binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Salary ~ ., family = binomial, data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-5.1013  -0.5296  -0.1926   0.0276   3.4349  \n",
       "\n",
       "Coefficients:\n",
       "                                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)                        -7.614e+00  4.525e-01 -16.826  < 2e-16 ***\n",
       "Age                                 2.626e-02  1.779e-03  14.762  < 2e-16 ***\n",
       "WorkClassLocal-gov                 -7.214e-01  1.168e-01  -6.179 6.46e-10 ***\n",
       "WorkClassPrivate                   -4.734e-01  9.693e-02  -4.884 1.04e-06 ***\n",
       "WorkClassSelf-emp-inc              -2.974e-01  1.283e-01  -2.317 0.020506 *  \n",
       "WorkClassSelf-emp-not-inc          -9.994e-01  1.139e-01  -8.772  < 2e-16 ***\n",
       "WorkClassState-gov                 -7.757e-01  1.294e-01  -5.996 2.03e-09 ***\n",
       "FinalWeight                         7.896e-07  1.822e-07   4.334 1.46e-05 ***\n",
       "Education11th                       6.909e-02  2.201e-01   0.314 0.753589    \n",
       "Education12th                       5.005e-01  2.940e-01   1.702 0.088676 .  \n",
       "Education7th-8th                   -6.213e-01  2.592e-01  -2.397 0.016530 *  \n",
       "Education9th                       -2.472e-01  2.856e-01  -0.865 0.386877    \n",
       "EducationAssoc-acdm                 1.302e+00  1.843e-01   7.066 1.60e-12 ***\n",
       "EducationAssoc-voc                  1.263e+00  1.772e-01   7.127 1.02e-12 ***\n",
       "EducationBachelors                  1.931e+00  1.647e-01  11.724  < 2e-16 ***\n",
       "EducationDoctorate                  3.076e+00  2.380e-01  12.926  < 2e-16 ***\n",
       "EducationHS-grad                    7.790e-01  1.598e-01   4.874 1.09e-06 ***\n",
       "EducationMasters                    2.319e+00  1.767e-01  13.126  < 2e-16 ***\n",
       "EducationProf-school                2.874e+00  2.145e-01  13.396  < 2e-16 ***\n",
       "EducationSome-college               1.108e+00  1.622e-01   6.832 8.36e-12 ***\n",
       "MaritalStatusMarried-civ-spouse     2.345e+00  3.050e-01   7.687 1.51e-14 ***\n",
       "MaritalStatusMarried-spouse-absent -3.345e-02  2.697e-01  -0.124 0.901286    \n",
       "MaritalStatusNever-married         -4.513e-01  9.187e-02  -4.912 9.01e-07 ***\n",
       "MaritalStatusSeparated             -9.621e-02  1.733e-01  -0.555 0.578829    \n",
       "MaritalStatusWidowed                1.484e-01  1.656e-01   0.896 0.370163    \n",
       "OccupationCraft-repair              5.906e-02  8.379e-02   0.705 0.480884    \n",
       "OccupationExec-managerial           7.693e-01  8.089e-02   9.511  < 2e-16 ***\n",
       "OccupationFarming-fishing          -9.919e-01  1.457e-01  -6.805 1.01e-11 ***\n",
       "OccupationHandlers-cleaners        -7.641e-01  1.529e-01  -4.999 5.77e-07 ***\n",
       "OccupationMachine-op-inspct        -2.794e-01  1.073e-01  -2.605 0.009191 ** \n",
       "OccupationOther-service            -8.967e-01  1.300e-01  -6.900 5.22e-12 ***\n",
       "OccupationProf-specialty            4.654e-01  8.613e-02   5.403 6.54e-08 ***\n",
       "OccupationProtective-serv           6.229e-01  1.302e-01   4.784 1.72e-06 ***\n",
       "OccupationSales                     2.770e-01  8.625e-02   3.211 0.001322 ** \n",
       "OccupationTech-support              6.359e-01  1.159e-01   5.488 4.07e-08 ***\n",
       "OccupationTransport-moving         -1.027e-01  1.032e-01  -0.995 0.319541    \n",
       "RelationshipNot-in-family           6.652e-01  3.021e-01   2.202 0.027683 *  \n",
       "RelationshipOther-relative         -4.067e-01  2.918e-01  -1.394 0.163395    \n",
       "RelationshipOwn-child              -6.044e-01  2.943e-01  -2.054 0.039994 *  \n",
       "RelationshipUnmarried               5.707e-01  3.174e-01   1.798 0.072185 .  \n",
       "RelationshipWife                    1.332e+00  1.103e-01  12.071  < 2e-16 ***\n",
       "RaceAsian-Pac-Islander              9.879e-01  2.997e-01   3.296 0.000979 ***\n",
       "RaceBlack                           3.929e-01  2.427e-01   1.619 0.105400    \n",
       "RaceOther                           1.524e-01  4.397e-01   0.347 0.728862    \n",
       "RaceWhite                           5.396e-01  2.305e-01   2.340 0.019266 *  \n",
       "GenderMale                          8.679e-01  8.403e-02  10.328  < 2e-16 ***\n",
       "CapitalGain                         3.191e-04  1.107e-05  28.830  < 2e-16 ***\n",
       "CapitalLoss                         6.503e-04  3.999e-05  16.264  < 2e-16 ***\n",
       "HoursWork                           2.965e-02  1.774e-03  16.714  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 31005  on 27244  degrees of freedom\n",
       "Residual deviance: 17976  on 27196  degrees of freedom\n",
       "AIC: 18074\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain statistics of the GLM model\n",
    "summary(glm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Selecting important feature against 0.05 threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return important features against 0.05 significant level\n",
    "get_important_features = function(model){\n",
    "    \n",
    "    # store pvalue and predictor as data frame\n",
    "    df_imp_features = data.frame(summary(model)$coef[(summary(model)$coef[,4] < 0.05), 4])\n",
    "    \n",
    "    # rename column\n",
    "    names(df_imp_features) = \"p_value\"\n",
    "    \n",
    "    # return pvalue and predictor as data frame\n",
    "    return (df_imp_features)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>p_value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td> 1.561804e-63</td></tr>\n",
       "\t<tr><th scope=row>Age</th><td> 2.591338e-49</td></tr>\n",
       "\t<tr><th scope=row>WorkClassLocal-gov</th><td> 6.457094e-10</td></tr>\n",
       "\t<tr><th scope=row>WorkClassPrivate</th><td> 1.039016e-06</td></tr>\n",
       "\t<tr><th scope=row>WorkClassSelf-emp-inc</th><td> 2.050583e-02</td></tr>\n",
       "\t<tr><th scope=row>WorkClassSelf-emp-not-inc</th><td> 1.747638e-18</td></tr>\n",
       "\t<tr><th scope=row>WorkClassState-gov</th><td> 2.026573e-09</td></tr>\n",
       "\t<tr><th scope=row>FinalWeight</th><td> 1.464846e-05</td></tr>\n",
       "\t<tr><th scope=row>Education7th-8th</th><td> 1.653000e-02</td></tr>\n",
       "\t<tr><th scope=row>EducationAssoc-acdm</th><td> 1.599694e-12</td></tr>\n",
       "\t<tr><th scope=row>EducationAssoc-voc</th><td> 1.023955e-12</td></tr>\n",
       "\t<tr><th scope=row>EducationBachelors</th><td> 9.636007e-32</td></tr>\n",
       "\t<tr><th scope=row>EducationDoctorate</th><td> 3.224490e-38</td></tr>\n",
       "\t<tr><th scope=row>EducationHS-grad</th><td> 1.094343e-06</td></tr>\n",
       "\t<tr><th scope=row>EducationMasters</th><td> 2.349945e-39</td></tr>\n",
       "\t<tr><th scope=row>EducationProf-school</th><td> 6.362770e-41</td></tr>\n",
       "\t<tr><th scope=row>EducationSome-college</th><td> 8.363157e-12</td></tr>\n",
       "\t<tr><th scope=row>MaritalStatusMarried-civ-spouse</th><td> 1.510914e-14</td></tr>\n",
       "\t<tr><th scope=row>MaritalStatusNever-married</th><td> 9.009754e-07</td></tr>\n",
       "\t<tr><th scope=row>OccupationExec-managerial</th><td> 1.887085e-21</td></tr>\n",
       "\t<tr><th scope=row>OccupationFarming-fishing</th><td> 1.007389e-11</td></tr>\n",
       "\t<tr><th scope=row>OccupationHandlers-cleaners</th><td> 5.765385e-07</td></tr>\n",
       "\t<tr><th scope=row>OccupationMachine-op-inspct</th><td> 9.191178e-03</td></tr>\n",
       "\t<tr><th scope=row>OccupationOther-service</th><td> 5.217529e-12</td></tr>\n",
       "\t<tr><th scope=row>OccupationProf-specialty</th><td> 6.543650e-08</td></tr>\n",
       "\t<tr><th scope=row>OccupationProtective-serv</th><td> 1.718776e-06</td></tr>\n",
       "\t<tr><th scope=row>OccupationSales</th><td> 1.321770e-03</td></tr>\n",
       "\t<tr><th scope=row>OccupationTech-support</th><td> 4.065421e-08</td></tr>\n",
       "\t<tr><th scope=row>RelationshipNot-in-family</th><td> 2.768284e-02</td></tr>\n",
       "\t<tr><th scope=row>RelationshipOwn-child</th><td> 3.999385e-02</td></tr>\n",
       "\t<tr><th scope=row>RelationshipWife</th><td> 1.499247e-33</td></tr>\n",
       "\t<tr><th scope=row>RaceAsian-Pac-Islander</th><td> 9.792498e-04</td></tr>\n",
       "\t<tr><th scope=row>RaceWhite</th><td> 1.926637e-02</td></tr>\n",
       "\t<tr><th scope=row>GenderMale</th><td> 5.272260e-25</td></tr>\n",
       "\t<tr><th scope=row>CapitalGain</th><td>9.086632e-183</td></tr>\n",
       "\t<tr><th scope=row>CapitalLoss</th><td> 1.791718e-59</td></tr>\n",
       "\t<tr><th scope=row>HoursWork</th><td> 1.033315e-62</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & p\\_value\\\\\n",
       "\\hline\n",
       "\t(Intercept) &  1.561804e-63\\\\\n",
       "\tAge &  2.591338e-49\\\\\n",
       "\tWorkClassLocal-gov &  6.457094e-10\\\\\n",
       "\tWorkClassPrivate &  1.039016e-06\\\\\n",
       "\tWorkClassSelf-emp-inc &  2.050583e-02\\\\\n",
       "\tWorkClassSelf-emp-not-inc &  1.747638e-18\\\\\n",
       "\tWorkClassState-gov &  2.026573e-09\\\\\n",
       "\tFinalWeight &  1.464846e-05\\\\\n",
       "\tEducation7th-8th &  1.653000e-02\\\\\n",
       "\tEducationAssoc-acdm &  1.599694e-12\\\\\n",
       "\tEducationAssoc-voc &  1.023955e-12\\\\\n",
       "\tEducationBachelors &  9.636007e-32\\\\\n",
       "\tEducationDoctorate &  3.224490e-38\\\\\n",
       "\tEducationHS-grad &  1.094343e-06\\\\\n",
       "\tEducationMasters &  2.349945e-39\\\\\n",
       "\tEducationProf-school &  6.362770e-41\\\\\n",
       "\tEducationSome-college &  8.363157e-12\\\\\n",
       "\tMaritalStatusMarried-civ-spouse &  1.510914e-14\\\\\n",
       "\tMaritalStatusNever-married &  9.009754e-07\\\\\n",
       "\tOccupationExec-managerial &  1.887085e-21\\\\\n",
       "\tOccupationFarming-fishing &  1.007389e-11\\\\\n",
       "\tOccupationHandlers-cleaners &  5.765385e-07\\\\\n",
       "\tOccupationMachine-op-inspct &  9.191178e-03\\\\\n",
       "\tOccupationOther-service &  5.217529e-12\\\\\n",
       "\tOccupationProf-specialty &  6.543650e-08\\\\\n",
       "\tOccupationProtective-serv &  1.718776e-06\\\\\n",
       "\tOccupationSales &  1.321770e-03\\\\\n",
       "\tOccupationTech-support &  4.065421e-08\\\\\n",
       "\tRelationshipNot-in-family &  2.768284e-02\\\\\n",
       "\tRelationshipOwn-child &  3.999385e-02\\\\\n",
       "\tRelationshipWife &  1.499247e-33\\\\\n",
       "\tRaceAsian-Pac-Islander &  9.792498e-04\\\\\n",
       "\tRaceWhite &  1.926637e-02\\\\\n",
       "\tGenderMale &  5.272260e-25\\\\\n",
       "\tCapitalGain & 9.086632e-183\\\\\n",
       "\tCapitalLoss &  1.791718e-59\\\\\n",
       "\tHoursWork &  1.033315e-62\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | p_value |\n",
       "|---|---|\n",
       "| (Intercept) |  1.561804e-63 |\n",
       "| Age |  2.591338e-49 |\n",
       "| WorkClassLocal-gov |  6.457094e-10 |\n",
       "| WorkClassPrivate |  1.039016e-06 |\n",
       "| WorkClassSelf-emp-inc |  2.050583e-02 |\n",
       "| WorkClassSelf-emp-not-inc |  1.747638e-18 |\n",
       "| WorkClassState-gov |  2.026573e-09 |\n",
       "| FinalWeight |  1.464846e-05 |\n",
       "| Education7th-8th |  1.653000e-02 |\n",
       "| EducationAssoc-acdm |  1.599694e-12 |\n",
       "| EducationAssoc-voc |  1.023955e-12 |\n",
       "| EducationBachelors |  9.636007e-32 |\n",
       "| EducationDoctorate |  3.224490e-38 |\n",
       "| EducationHS-grad |  1.094343e-06 |\n",
       "| EducationMasters |  2.349945e-39 |\n",
       "| EducationProf-school |  6.362770e-41 |\n",
       "| EducationSome-college |  8.363157e-12 |\n",
       "| MaritalStatusMarried-civ-spouse |  1.510914e-14 |\n",
       "| MaritalStatusNever-married |  9.009754e-07 |\n",
       "| OccupationExec-managerial |  1.887085e-21 |\n",
       "| OccupationFarming-fishing |  1.007389e-11 |\n",
       "| OccupationHandlers-cleaners |  5.765385e-07 |\n",
       "| OccupationMachine-op-inspct |  9.191178e-03 |\n",
       "| OccupationOther-service |  5.217529e-12 |\n",
       "| OccupationProf-specialty |  6.543650e-08 |\n",
       "| OccupationProtective-serv |  1.718776e-06 |\n",
       "| OccupationSales |  1.321770e-03 |\n",
       "| OccupationTech-support |  4.065421e-08 |\n",
       "| RelationshipNot-in-family |  2.768284e-02 |\n",
       "| RelationshipOwn-child |  3.999385e-02 |\n",
       "| RelationshipWife |  1.499247e-33 |\n",
       "| RaceAsian-Pac-Islander |  9.792498e-04 |\n",
       "| RaceWhite |  1.926637e-02 |\n",
       "| GenderMale |  5.272260e-25 |\n",
       "| CapitalGain | 9.086632e-183 |\n",
       "| CapitalLoss |  1.791718e-59 |\n",
       "| HoursWork |  1.033315e-62 |\n",
       "\n"
      ],
      "text/plain": [
       "                                p_value      \n",
       "(Intercept)                      1.561804e-63\n",
       "Age                              2.591338e-49\n",
       "WorkClassLocal-gov               6.457094e-10\n",
       "WorkClassPrivate                 1.039016e-06\n",
       "WorkClassSelf-emp-inc            2.050583e-02\n",
       "WorkClassSelf-emp-not-inc        1.747638e-18\n",
       "WorkClassState-gov               2.026573e-09\n",
       "FinalWeight                      1.464846e-05\n",
       "Education7th-8th                 1.653000e-02\n",
       "EducationAssoc-acdm              1.599694e-12\n",
       "EducationAssoc-voc               1.023955e-12\n",
       "EducationBachelors               9.636007e-32\n",
       "EducationDoctorate               3.224490e-38\n",
       "EducationHS-grad                 1.094343e-06\n",
       "EducationMasters                 2.349945e-39\n",
       "EducationProf-school             6.362770e-41\n",
       "EducationSome-college            8.363157e-12\n",
       "MaritalStatusMarried-civ-spouse  1.510914e-14\n",
       "MaritalStatusNever-married       9.009754e-07\n",
       "OccupationExec-managerial        1.887085e-21\n",
       "OccupationFarming-fishing        1.007389e-11\n",
       "OccupationHandlers-cleaners      5.765385e-07\n",
       "OccupationMachine-op-inspct      9.191178e-03\n",
       "OccupationOther-service          5.217529e-12\n",
       "OccupationProf-specialty         6.543650e-08\n",
       "OccupationProtective-serv        1.718776e-06\n",
       "OccupationSales                  1.321770e-03\n",
       "OccupationTech-support           4.065421e-08\n",
       "RelationshipNot-in-family        2.768284e-02\n",
       "RelationshipOwn-child            3.999385e-02\n",
       "RelationshipWife                 1.499247e-33\n",
       "RaceAsian-Pac-Islander           9.792498e-04\n",
       "RaceWhite                        1.926637e-02\n",
       "GenderMale                       5.272260e-25\n",
       "CapitalGain                     9.086632e-183\n",
       "CapitalLoss                      1.791718e-59\n",
       "HoursWork                        1.033315e-62"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the important features of the glm.fit model\n",
    "imp_features = get_important_features(glm.fit)\n",
    "imp_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the important features against 0.05 significance, we can say that the below 36 predictors are possibly associated with Salary.\n",
    "1. 'Age' \n",
    "2. 'WorkClassLocal-gov' \n",
    "3. 'WorkClassPrivate' \n",
    "4. 'WorkClassSelf-emp-inc' \n",
    "5. 'WorkClassSelf-emp-not-inc' \n",
    "6. 'WorkClassState-gov' \n",
    "7. 'FinalWeight' \n",
    "8. 'Education7th-8th' \n",
    "9. 'EducationAssoc-acdm' \n",
    "10. 'EducationAssoc-voc' \n",
    "11. 'EducationBachelors' \n",
    "12. 'EducationDoctorate' \n",
    "13. 'EducationHS-grad' \n",
    "14. 'EducationMasters' \n",
    "15. 'EducationProf-school' \n",
    "16. 'EducationSome-college' \n",
    "17. 'MaritalStatusMarried-civ-spouse' \n",
    "18. 'MaritalStatusNever-married' \n",
    "19. 'OccupationExec-managerial' \n",
    "20. 'OccupationFarming-fishing' \n",
    "21. 'OccupationHandlers-cleaners' \n",
    "22. 'OccupationMachine-op-inspct' \n",
    "23. 'OccupationOther-service' \n",
    "24. 'OccupationProf-specialty' \n",
    "25. 'OccupationProtective-serv' \n",
    "26. 'OccupationSales' \n",
    "27. 'OccupationTech-support' \n",
    "28. 'RelationshipNot-in-family' \n",
    "29. 'RelationshipOwn-child' \n",
    "30. 'RelationshipWife' \n",
    "31. 'RaceAsian-Pac-Islander' \n",
    "32. 'RaceWhite' \n",
    "33. 'GenderMale' \n",
    "34. 'CapitalGain' \n",
    "35. 'CapitalLoss' \n",
    "36. 'HoursWork'\n",
    "\n",
    "This is because the p value for these predictors is very small which suggests a relation between the predictors and the target variable Salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **2.Selecting three most important features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the top 3 predictors\n",
    "get_top_3_predictors = function(model_summary){\n",
    "    \n",
    "    # remove 'Intercept' as it is not a predictor of the model\n",
    "    df_pvalue = subset(model_summary, rownames(model_summary) != '(Intercept)')\n",
    "    \n",
    "    # sort the df according to p_value\n",
    "    lst = sort(df_pvalue[,1], index.return=TRUE, decreasing=FALSE)\n",
    "    \n",
    "    # get the top 3 predictors after sort\n",
    "    imp_3_features = lapply(lst, `[`,lst$x %in% head(unique(lst$x),3))\n",
    "    \n",
    "    # return the name of the top 3 predictors\n",
    "    return (rownames(df_pvalue)[imp_3_features$ix])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'CapitalGain'</li>\n",
       "\t<li>'HoursWork'</li>\n",
       "\t<li>'CapitalLoss'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'CapitalGain'\n",
       "\\item 'HoursWork'\n",
       "\\item 'CapitalLoss'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'CapitalGain'\n",
       "2. 'HoursWork'\n",
       "3. 'CapitalLoss'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"CapitalGain\" \"HoursWork\"   \"CapitalLoss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the top 3 predictors of our 'glm' model\n",
    "get_top_3_predictors(imp_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the top 3 most important predictors we see the smallest p-value. A small p-value suggests a relation between the predictors and the target variable.\n",
    "\n",
    "'CapitalGain', 'HoursWork' and 'CapitalLoss' have the smallest p-values. Therefore, these are the top 3 most signinficant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUu5eFpOWJi"
   },
   "source": [
    "<h2> Question 2 (10 Marks) </h2>\n",
    "\n",
    "Firstly, please use the model created in the previous question to predict for the labels of the $\\textbf{train}$ data. Consequently, our objective is to compare this ``predict.label`` with the ``truth.label`` from the $\\textbf{test}$ data. However, as we don't know the $\\textbf{test}$ label, we have to estimate model performance using $\\textbf{train}$ data at this moment.\n",
    "\n",
    "Secondly, since our objective is to estimate the performance of this model in making correct predictions; thus, this question also asks you to explore different [performance metrics](https://en.wikipedia.org/wiki/Precision_and_recall) for classification models. The metrics we will use are $\\textbf{Accuracy, Precision, Recall, and F1 Score}$, please create a function to calculate these value and print them out properly using the given structure.\n",
    "\n",
    "Additionally, please also discuss the results of these values in the context of your model.\n",
    "\n",
    "$\\textbf{Note}$: This asks for your descriptions, please refrain from using one or two lines to describe/discuss the effect. Keep answers to be 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "$\\textbf{YOUR ANSWER HERE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your previous model to perform prediction, keep type = \"response\"\n",
    "# Don't worry if you receive some warnings, they are benign\n",
    "predict.label <- predict(glm.fit, train, type=\"response\")\n",
    "# Truth label from train data\n",
    "truth.label <- train$Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model statistics function\n",
    "mod.stat <- function(predict.label, truth.label){\n",
    "    # instantiate the variables\n",
    "    accuracy <- NULL\n",
    "    precision <- NULL\n",
    "    recall <- NULL\n",
    "    F1 <- NULL\n",
    "    \n",
    "    # convert to TRUE and FALSE\n",
    "    predict.label = ifelse(predict.label >= 0.5, TRUE, FALSE)\n",
    "    \n",
    "    # accuracy\n",
    "    accuracy = mean(predict.label == truth.label)\n",
    "    \n",
    "    # True positives\n",
    "    true_positives  = sum( (predict.label == TRUE ) & (truth.label == TRUE ) )\n",
    "    # False positives\n",
    "    false_positives = sum( (predict.label == TRUE) & (truth.label == FALSE) )\n",
    "    \n",
    "    # Precision\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    \n",
    "    # False negatives\n",
    "    false_negatives = sum( (predict.label == FALSE) & (truth.label == TRUE) )\n",
    "    \n",
    "    # Recall\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    # f score\n",
    "    F1 = 2*precision*recall / (precision+recall)\n",
    "    \n",
    "    # Return a list of value\n",
    "    return(list(\"accuracy\" = round(accuracy,4), \"precision\" = round(precision,4), \"recall\" = round(recall,4), \"fscore\" = round(F1,4)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8452</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.7395</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.6107</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.669</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8452\n",
       "\\item[\\$precision] 0.7395\n",
       "\\item[\\$recall] 0.6107\n",
       "\\item[\\$fscore] 0.669\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8452\n",
       "$precision\n",
       ":   0.7395\n",
       "$recall\n",
       ":   0.6107\n",
       "$fscore\n",
       ":   0.669\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8452\n",
       "\n",
       "$precision\n",
       "[1] 0.7395\n",
       "\n",
       "$recall\n",
       "[1] 0.6107\n",
       "\n",
       "$fscore\n",
       "[1] 0.669\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the function to get statistics, provide description/discussion after this\n",
    "mod.stat(predict.label, truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy :**\n",
    "\n",
    "The Generalised Linear Regression model classifies the test data corretly 84.52% of the times. This means it predicts TRUE whens the gorund truth is TRUE, and predicts FALSE when ground truth is FALSE for 84.52% of the times of the test dataset.\n",
    "\n",
    "**Precision :**\n",
    "\n",
    "Precision of our model is 73.95%. This means of all the data classified as TRUE by the model, the prediction was correct for 73.95% of the times. In our case, we correctly identify salary being > 50K out of all the person actually having this to be true. \n",
    "\n",
    "**Recall :**\n",
    "\n",
    "Recall of the model our model is 61.07%. This means, that the prediction was TRUE for 61.07% of the times with the ground truth. In our case, the model was able to classsiy the salary > 50K correcly for 61.07% of the times. \n",
    "\n",
    "**fscore :**\n",
    "\n",
    "Both precision and recall independently are not good to test our model. A model can have a good precision and bad recall, and vice versa. This issue can be solved using the fscore, which is the Harmonic means of the recall and precision. It provides a way to describe both the precision and recall in one metric.\n",
    "\n",
    "Our model has a fscore of 0.669 which is not bad. A good fscore is 1 and a bad fcore is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XyKtwfOWJj"
   },
   "source": [
    "<h2> Question 3 (5 Marks) </h2>\n",
    "\n",
    "Use the stepwise selection procedure with the $\\textbf{BIC}$ penalty to prune out potentially unimportant variables. Checking the performance of your model using the created ``mod.stat()`` function, please give your discussion as how this model is compared with the ``glm.fit``(you can run the ``mod.stat()`` function for this as well if you want to).\n",
    "\n",
    "$\\textbf{Note}$: please don't change the default direction ``both`` in the step function, this is so that we can check your work easily. Additionally, please name this model ``sw.fit``. Don't worry about the warnings, they are benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Salary ~ Age + WorkClass + FinalWeight + Education + \n",
       "    MaritalStatus + Occupation + Relationship + Gender + CapitalGain + \n",
       "    CapitalLoss + HoursWork, family = binomial, data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-5.0961  -0.5291  -0.1936   0.0279   3.4393  \n",
       "\n",
       "Coefficients:\n",
       "                                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)                        -7.130e+00  3.929e-01 -18.146  < 2e-16 ***\n",
       "Age                                 2.644e-02  1.778e-03  14.869  < 2e-16 ***\n",
       "WorkClassLocal-gov                 -7.159e-01  1.163e-01  -6.154 7.54e-10 ***\n",
       "WorkClassPrivate                   -4.588e-01  9.626e-02  -4.766 1.88e-06 ***\n",
       "WorkClassSelf-emp-inc              -2.776e-01  1.278e-01  -2.173  0.02977 *  \n",
       "WorkClassSelf-emp-not-inc          -9.857e-01  1.133e-01  -8.703  < 2e-16 ***\n",
       "WorkClassState-gov                 -7.653e-01  1.290e-01  -5.930 3.02e-09 ***\n",
       "FinalWeight                         7.496e-07  1.800e-07   4.164 3.13e-05 ***\n",
       "Education11th                       7.190e-02  2.201e-01   0.327  0.74395    \n",
       "Education12th                       5.065e-01  2.939e-01   1.724  0.08479 .  \n",
       "Education7th-8th                   -6.220e-01  2.593e-01  -2.399  0.01643 *  \n",
       "Education9th                       -2.417e-01  2.851e-01  -0.848  0.39663    \n",
       "EducationAssoc-acdm                 1.323e+00  1.841e-01   7.187 6.60e-13 ***\n",
       "EducationAssoc-voc                  1.277e+00  1.770e-01   7.215 5.38e-13 ***\n",
       "EducationBachelors                  1.947e+00  1.645e-01  11.838  < 2e-16 ***\n",
       "EducationDoctorate                  3.089e+00  2.377e-01  12.998  < 2e-16 ***\n",
       "EducationHS-grad                    7.881e-01  1.596e-01   4.937 7.95e-07 ***\n",
       "EducationMasters                    2.337e+00  1.765e-01  13.239  < 2e-16 ***\n",
       "EducationProf-school                2.894e+00  2.145e-01  13.495  < 2e-16 ***\n",
       "EducationSome-college               1.117e+00  1.621e-01   6.893 5.47e-12 ***\n",
       "MaritalStatusMarried-civ-spouse     2.357e+00  3.045e-01   7.743 9.75e-15 ***\n",
       "MaritalStatusMarried-spouse-absent -3.049e-02  2.690e-01  -0.113  0.90977    \n",
       "MaritalStatusNever-married         -4.482e-01  9.172e-02  -4.886 1.03e-06 ***\n",
       "MaritalStatusSeparated             -1.101e-01  1.728e-01  -0.637  0.52405    \n",
       "MaritalStatusWidowed                1.497e-01  1.655e-01   0.904  0.36583    \n",
       "OccupationCraft-repair              6.350e-02  8.372e-02   0.759  0.44812    \n",
       "OccupationExec-managerial           7.726e-01  8.083e-02   9.558  < 2e-16 ***\n",
       "OccupationFarming-fishing          -9.869e-01  1.456e-01  -6.779 1.21e-11 ***\n",
       "OccupationHandlers-cleaners        -7.678e-01  1.528e-01  -5.026 5.02e-07 ***\n",
       "OccupationMachine-op-inspct        -2.857e-01  1.072e-01  -2.665  0.00770 ** \n",
       "OccupationOther-service            -9.059e-01  1.298e-01  -6.981 2.92e-12 ***\n",
       "OccupationProf-specialty            4.656e-01  8.599e-02   5.414 6.15e-08 ***\n",
       "OccupationProtective-serv           6.192e-01  1.301e-01   4.760 1.94e-06 ***\n",
       "OccupationSales                     2.797e-01  8.618e-02   3.246  0.00117 ** \n",
       "OccupationTech-support              6.444e-01  1.157e-01   5.568 2.57e-08 ***\n",
       "OccupationTransport-moving         -1.082e-01  1.031e-01  -1.050  0.29391    \n",
       "RelationshipNot-in-family           6.761e-01  3.016e-01   2.242  0.02499 *  \n",
       "RelationshipOther-relative         -4.031e-01  2.920e-01  -1.381  0.16742    \n",
       "RelationshipOwn-child              -5.877e-01  2.935e-01  -2.002  0.04525 *  \n",
       "RelationshipUnmarried               5.744e-01  3.168e-01   1.813  0.06984 .  \n",
       "RelationshipWife                    1.332e+00  1.103e-01  12.076  < 2e-16 ***\n",
       "GenderMale                          8.747e-01  8.401e-02  10.412  < 2e-16 ***\n",
       "CapitalGain                         3.184e-04  1.106e-05  28.784  < 2e-16 ***\n",
       "CapitalLoss                         6.509e-04  3.998e-05  16.281  < 2e-16 ***\n",
       "HoursWork                           2.968e-02  1.774e-03  16.735  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 31005  on 27244  degrees of freedom\n",
       "Residual deviance: 17992  on 27200  degrees of freedom\n",
       "AIC: 18082\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting to suppress warnings\n",
    "options(warn=-1)\n",
    "\n",
    "# Fit a stepwise model\n",
    "sw.fit <- step(glm.fit,k=log(nrow(train)),trace=0,direction = c(\"both\"))\n",
    "\n",
    "# Setting to suppress warnings\n",
    "options(warn=0)\n",
    "\n",
    "# Getting the summary to understand the result\n",
    "summary(sw.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8451</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.7396</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.6104</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.6688</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8451\n",
       "\\item[\\$precision] 0.7396\n",
       "\\item[\\$recall] 0.6104\n",
       "\\item[\\$fscore] 0.6688\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8451\n",
       "$precision\n",
       ":   0.7396\n",
       "$recall\n",
       ":   0.6104\n",
       "$fscore\n",
       ":   0.6688\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8451\n",
       "\n",
       "$precision\n",
       "[1] 0.7396\n",
       "\n",
       "$recall\n",
       "[1] 0.6104\n",
       "\n",
       "$fscore\n",
       "[1] 0.6688\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making prediction using train data and view the statistics\n",
    "predict.label.sw <- predict(sw.fit, train, type=\"response\")\n",
    "\n",
    "# Only run the below if you have labels, in your submission, this must be UNCOMMENTED\n",
    "# Truth label from train data\n",
    "truth.label <- train$Salary\n",
    "\n",
    "# view statistics\n",
    "mod.stat(predict.label.sw, truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The prunned GLM model has almost similar values of Accuracy, Precision, Recall and Fscore. This means the prunned model is as good as the original GLM model. One other thing we can infer from this is that all the predictors of the originl model are important as we can see them in the prunned model as well.\n",
    "\n",
    "The old model had a accuracy of 84.52% while the prunned model has an accuracy of 84.51% which is slight a decrease. \n",
    "\n",
    "The old model had a precision of 73.95% while the prunned model has an precision of 73.96% which is slight a increase. This means the number of True positives increased or the False positives decreased.\n",
    "\n",
    "The old model had a recall of 61.07% while the prunned model has a recall of 61.04% which is slight a decrease. This means the number of True positives increased or the False negatives decreased.\n",
    "\n",
    "The old model had a fscore of 66.9% while the prunned model has a fscore of 66.88% which is slight a decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 4 (Libraries are allowed) (25 Marks) </h2>\n",
    "\n",
    "Similar to the first part, to simulate for a realistic modelling process, this question will be in the form of a competition among students to find out who has the best model.\n",
    "\n",
    "Thus, You will be graded by the performance of your model compared to your classmates', the better your model, the higher your score. Additionally, you need to write a short paragraph describing/documenting your thought process in this model building process ``(300 words)``. Note that this is to explain to us why you build your current model so that we can verify that you understand the model you build and not just copy from other people.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "``install.packages(\"some package\", repos='http://cran.us.r-project.org')``\n",
    "\n",
    "``library(\"some package\")``\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you a deduction, our suggestion is for you to use the standard ``R version 3.6.1``\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works perfectly.\n",
    "\n",
    "\n",
    "$20$ Marks for the model performance in the competition\n",
    "\n",
    "$5$ Marks for logically writing down the thought process in building the final model\n",
    "\n",
    "This is the [link](https://www.kaggle.com/t/1bdebc96607742dbaf47ab36cd3ae421) to the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{YOUR ANSWER HERE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The main objective of this task is to build a model that most appropriately classifies the Salary of a person to be greater than 50K. The outcome of our model is Binary, which mean its a Classification problem.\n",
    "\n",
    "To start with the Model building process, we first use the Generalized Linear Model which we prunned using BIC penalty. Using this model as the starting point, we run the predictions. This gives a mean accuracy value of around 0.8498.\n",
    "\n",
    "To improve our classification model, we try out different classification Algorithms to have a better model that most appropriately classifies the person salary.\n",
    "\n",
    "Following are the different algorithms I have tried for this Classification task with the respective approximate mean F Score values obatined:\n",
    "\n",
    "1. Generalised linear model (Prunned) - 0.84984\n",
    "2. Linear Discriminant Analysis (LDA) - 0.84397\n",
    "3. Classification and Regression Trees  - 0.81193\n",
    "4. kNN - 0.78429\n",
    "5. Boosted regression trees - 0.88334\n",
    "6. Random Forest - 0.8362\n",
    "\n",
    "Looking at the above mean Accuracy values, its best to use Boosted regression trees for this Classification task. Boosting is a iteractive learning way to improve the performance of Supervised learning algorithms in which the weight of the observation is based on the last classification. In this technique, the weight of incorreclty data is increased. The main goal is to learn from the error of the last tree and increase the accuracy. This technique is good as it removes the efforts of cleaning the dataset and learns the underlying non linear relationships between predictors.\n",
    "\n",
    "To have a robust classification model, we carry out the sampling process 10 times as specified in the `trainControl()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/prash/OneDrive/Documents/R/win-library/3.6'\n",
      "(as 'lib' is unspecified)\n",
      "Warning message:\n",
      "\"package 'xgboost' is in use and will not be installed\"Installing package into 'C:/Users/prash/OneDrive/Documents/R/win-library/3.6'\n",
      "(as 'lib' is unspecified)\n",
      "Warning message:\n",
      "\"package 'caret' is in use and will not be installed\""
     ]
    }
   ],
   "source": [
    "## install required packages\n",
    "install.packages(\"xgboost\")\n",
    "install.packages(\"caret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'xgboost' was built under R version 3.6.3\""
     ]
    }
   ],
   "source": [
    "# load the libraries\n",
    "library(caret)\n",
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the target variable as Factor\n",
    "train$Salary = as.factor(train$Salary)\n",
    "\n",
    "# Fit the model on the training set\n",
    "set.seed(123)\n",
    "fin.mod <- train(Salary ~., data = train, method = \"xgbTree\",trControl = trainControl(\"cv\", number = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.label <- predict(fin.mod, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Use this csv file to commit to the leaderboard\n",
    "write.csv(data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "          \"ClassPredictLabel.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "source(\"../data/modassess.r\")\n",
    "model.perf <- mod.stat.test(pred.label,label$Label)\n",
    "print(model.perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Va7-A1UviCmz"
   },
   "source": [
    "<h1>References</h1>\n",
    "\n",
    "Brownlee, J. (2020). How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification. Retrieved 9 November 2020, from https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "\n",
    "A complete guide to the random forest algorithm. (2020). Retrieved 9 November 2020, from https://builtin.com/data-science/random-forest-algorithm\n",
    "\n",
    "Classification Algorithms | Types of Classification Algorithms | Edureka. (2020). Retrieved 9 November 2020, from https://www.edureka.co/blog/classification-algorithms/\n",
    "\n",
    "Garg, R. (2020). A Primer to Ensemble Learning – Bagging and Boosting. Retrieved 9 November 2020, from https://analyticsindiamag.com/primer-ensemble-learning-bagging-boosting/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1 Instructions and Contents.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
